{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# !poetry export --without-hashes --dev -f requirements.txt -o requirements-dev.txt\n",
    "!pip install -U pip\n",
    "# chikkapy depends on dartsclone\n",
    "!pip install dartsclone\n",
    "!pip install -r requirements-dev.txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b51c0cbf-6c0d-40c8-ada8-1ea160d46ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "import gc\n",
    "import torch\n",
    "# Reset GPU memory\n",
    "# https://stackoverflow.com/questions/54374935/how-to-fix-this-strange-error-runtimeerror-cuda-error-out-of-memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "00cb424f-fba9-460f-ad22-d6f5f46a19b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "          Sentence  UserID          Datetime Train/Dev/Test  Writer_Joy  \\\n17254  花粉症の薬はじめました      21  2020/01/16 08:25          train           0   \n\n       Writer_Sadness  Writer_Anticipation  Writer_Surprise  Writer_Anger  \\\n17254               1                    2                0             0   \n\n       Writer_Fear  ...  Reader3_Disgust  Reader3_Trust  Avg. Readers_Joy  \\\n17254            0  ...                1              0                 0   \n\n       Avg. Readers_Sadness  Avg. Readers_Anticipation  Avg. Readers_Surprise  \\\n17254                     0                          1                      1   \n\n       Avg. Readers_Anger  Avg. Readers_Fear  Avg. Readers_Disgust  \\\n17254                   0                  0                     0   \n\n       Avg. Readers_Trust  \n17254                   0  \n\n[1 rows x 44 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence</th>\n      <th>UserID</th>\n      <th>Datetime</th>\n      <th>Train/Dev/Test</th>\n      <th>Writer_Joy</th>\n      <th>Writer_Sadness</th>\n      <th>Writer_Anticipation</th>\n      <th>Writer_Surprise</th>\n      <th>Writer_Anger</th>\n      <th>Writer_Fear</th>\n      <th>...</th>\n      <th>Reader3_Disgust</th>\n      <th>Reader3_Trust</th>\n      <th>Avg. Readers_Joy</th>\n      <th>Avg. Readers_Sadness</th>\n      <th>Avg. Readers_Anticipation</th>\n      <th>Avg. Readers_Surprise</th>\n      <th>Avg. Readers_Anger</th>\n      <th>Avg. Readers_Fear</th>\n      <th>Avg. Readers_Disgust</th>\n      <th>Avg. Readers_Trust</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>17254</th>\n      <td>花粉症の薬はじめました</td>\n      <td>21</td>\n      <td>2020/01/16 08:25</td>\n      <td>train</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 44 columns</p>\n</div>"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    BytesIO(urlopen(\"https://github.com/ids-cv/wrime/raw/master/wrime.tsv\").read())\n",
    "    , sep=\"\\t\"\n",
    ")\n",
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8b7dfb1c-3481-4861-931d-b5ae184d3b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Avg. Readers_Joy             13317\nAvg. Readers_Sadness         11194\nAvg. Readers_Anticipation    13831\nAvg. Readers_Surprise        10852\nAvg. Readers_Anger            1429\nAvg. Readers_Fear             9114\nAvg. Readers_Disgust          7521\nAvg. Readers_Trust            1918\ndtype: int64"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"This data has multiple sentiment label. \n",
    "For sake of simplicity, transform the task multi-label to binary classification.\n",
    "So, need to explore the label that seems to have most countable records (and it's `Joy`).\n",
    "\"\"\"\n",
    "\n",
    "df[[\n",
    "    c for c in df.columns if c.startswith(\"Avg. Readers_\")\n",
    "]].applymap(lambda row: 1 if row > 0 else 0).agg('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f757f4b9-c3a7-4daa-87b9-e769f17fa5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "targets = np.expand_dims(\n",
    "    df[\"Avg. Readers_Joy\"].apply(lambda row: 1 if row > 0 else 0),\n",
    "    axis=1\n",
    ")\n",
    "user_ids = df[\"UserID\"].values\n",
    "sentences = df[\"Sentence\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 2021\n",
    "record_idx = df.index.tolist()\n",
    "# record_idx = list(range(len(df)))\n",
    "_train_idx, test_idx, _, _ = train_test_split(record_idx, record_idx, test_size=0.2, random_state=seed,\n",
    "                                              stratify=user_ids)\n",
    "train_idx, valid_idx, _, _ = train_test_split(_train_idx, _train_idx, test_size=0.2, random_state=seed,\n",
    "                                              stratify=user_ids[_train_idx])\n",
    "\n",
    "# train_texts = sentences[train_idx]\n",
    "# valid_texts = sentences[valid_idx]\n",
    "# test_texts = sentences[test_idx]\n",
    "\n",
    "y_train = targets[train_idx]\n",
    "y_valid = targets[valid_idx]\n",
    "y_test = targets[test_idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# tips to speed up\n",
    "torch.backends.cudnn.benchmark = True\n",
    "# https://huggingface.co/cl-tohoku/bert-base-japanese-v2\n",
    "MODEL_TYPE = \"cl-tohoku/bert-base-japanese-v2\"\n",
    "MAX_LENGTH = 128\n",
    "LEARNING_RATE = 1e-5\n",
    "WARM_UP_RATIO = 0.1\n",
    "BATCH_SIZE = 64  # It's the best deal for GPU(T4). For more batch_size, stronger GPU.\n",
    "# N_EPOCHS = 50\n",
    "N_EPOCHS = 3\n",
    "NUM_WORKERS = os.cpu_count() - 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/43200 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ab2cc359fac48fcbebce55dbc1be90a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "32768"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_TYPE)\n",
    "tokens = [\n",
    "    tokenizer.encode_plus(sentence,\n",
    "                          add_special_tokens=True,\n",
    "                          max_length=MAX_LENGTH,\n",
    "                          padding='max_length',\n",
    "                          truncation=True,\n",
    "                          ) for sentence in tqdm(sentences)\n",
    "]\n",
    "\n",
    "train_tokens = [tokens[i] for i in train_idx]\n",
    "valid_tokens = [tokens[i] for i in valid_idx]\n",
    "test_tokens = [tokens[i] for i in test_idx]\n",
    "\n",
    "len(tokenizer.get_vocab())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [
    "class WrimeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokens, targets):\n",
    "        self.tokens = tokens\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        token = self.tokens[index]\n",
    "        target = self.targets[index]\n",
    "\n",
    "        input_ids = torch.tensor(token[\"input_ids\"])\n",
    "        attention_mask = torch.tensor(token[\"attention_mask\"])\n",
    "        token_type_ids = torch.tensor(token[\"token_type_ids\"])\n",
    "        target = torch.tensor(target).float()\n",
    "\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            target=target,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "ds_train = torch.utils.data.DataLoader(\n",
    "    WrimeDataset(train_tokens, y_train),\n",
    "    # batch_size=BATCH_SIZE, drop_last=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    batch_size=BATCH_SIZE, drop_last=True)\n",
    "ds_valid = torch.utils.data.DataLoader(\n",
    "    WrimeDataset(valid_tokens, y_valid),\n",
    "    # batch_size=BATCH_SIZE, drop_last=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    batch_size=BATCH_SIZE, drop_last=False)\n",
    "ds_test = torch.utils.data.DataLoader(\n",
    "    WrimeDataset(test_tokens, y_test),\n",
    "    # batch_size=BATCH_SIZE, drop_last=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    batch_size=BATCH_SIZE, drop_last=False)\n",
    "\n",
    "# iter(ds_test).__next__()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from transformers import (\n",
    "    BertConfig, AutoModel, AdamW, get_cosine_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, model_type, tokenizer, num_classes):\n",
    "        super().__init__()\n",
    "        config = BertConfig(model_type)\n",
    "        config.vocab_size = tokenizer.vocab_size\n",
    "        self.bert = AutoModel.from_pretrained(model_type, config=config)\n",
    "        self.fc1 = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.fc2 = nn.Linear(config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # https://stackoverflow.com/a/67352953/9489217\n",
    "        _, h = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, return_dict=False)\n",
    "        h = nn.ReLU()(h)\n",
    "        h = self.fc1(h)\n",
    "        h = nn.ReLU()(h)\n",
    "        h = self.fc2(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "model = BertClassifier(MODEL_TYPE, tokenizer=tokenizer, num_classes=1)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "num_train_steps = len(ds_train) * N_EPOCHS\n",
    "num_warmup_steps = int(num_train_steps * WARM_UP_RATIO)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps,\n",
    "                                         num_training_steps=num_train_steps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def train_loop(ds, model, optimizer, scheduler, device):\n",
    "    losses, learning_rates = [], []\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    for row in tqdm(ds, total=len(ds)):\n",
    "        input_ids = row[\"input_ids\"].to(device)\n",
    "        attention_mask = row[\"attention_mask\"].to(device)\n",
    "        token_type_ids = row[\"token_type_ids\"].to(device)\n",
    "        target = row[\"target\"].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        output = model(input_ids, attention_mask, token_type_ids)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        learning_rates.append(np.array([\n",
    "            pm[\"lr\"] for pm in optimizer.param_groups\n",
    "        ]).mean())\n",
    "        losses.append(loss.item())\n",
    "    return learning_rates, losses, model\n",
    "\n",
    "def test_loop(ds, model, device):\n",
    "    losses, predicts = [], []\n",
    "    model.eval()\n",
    "    for row in tqdm(ds, total=len(ds)):\n",
    "        input_ids = row[\"input_ids\"].to(device)\n",
    "        attention_mask = row[\"attention_mask\"].to(device)\n",
    "        token_type_ids = row[\"token_type_ids\"].to(device)\n",
    "        target = row[\"target\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "        loss = loss_fn(output, target)\n",
    "        losses.append(loss.item())\n",
    "        predicts += output.sigmoid().cpu().tolist()\n",
    "    return predicts, np.array(losses).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "log_dir = \"runs/baseline_exp1\"\n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir $log_dir"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorboard import notebook\n",
    "notebook.list()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# notebook.display(port=6006, height=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f\"Epoch-{epoch}\")\n",
    "    train_loop(ds_train, model, optimizer, scheduler, device)\n",
    "\n",
    "    y_pred, val_loss = test_loop(ds_valid, model, device)\n",
    "    val_acc = accuracy_score(y_valid, y_pred)\n",
    "\n",
    "    y_pred, test_loss = test_loop(ds_test, model, device)\n",
    "    test_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    writer.add_scalar(\"Loss/train\", val_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/test\", test_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy/train\", val_acc, epoch)\n",
    "    writer.add_scalar(\"Accuracy/test\", test_acc, epoch)\n",
    "\n",
    "    print(f\"\\tvalid:\\tloss={val_loss}/score={val_acc}\")\n",
    "    print(f\"\\ttest:\\tloss={test_loss}/score={test_acc}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}